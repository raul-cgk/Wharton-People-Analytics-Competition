{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wharton - People Analytics Conference\n",
    "#### Teach For America\n",
    "**The data used for this project is confidential and owned by TFA so all outputs have been cleared out.**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I. Case Questions\n",
    "\n",
    "1. How do characteristics of the applicant relate to whether the candidate arrives for Day 1 training and completes the program through Year 2? From information available to TFA before the final interview, predict the candidate outcomes at Year 2.\n",
    "\n",
    "2. In what way, if at all, is cost-of-living (as provided in the data) related to candidate decisions?\n",
    "\n",
    "3. How could TFA refine who they invite to interview to identify those who, if admitted, would be highly likely to begin training and complete their corps?\n",
    "\n",
    "4. Implications of your analysis: Based on your analysis, what recruiting initiatives would you recommend to help TFA maximize the probability of interviewed candidates attending training and matriculating through Year 2 while accounting for candidate quality and geographic need?\n",
    "\n",
    "5. We have 95k who recieve an interview invite and then we have 57k who complete the interview. Do we know how many showed up to the interview of the people who recieved an interview invitation?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Data Collection + Cleanup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from scipy.stats import chi2_contingency\n",
    "from scipy.stats import ks_2samp\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import learning_curve \n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.inspection import permutation_importance\n",
    "from scipy.stats import mannwhitneyu\n",
    "from collections import Counter\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the data\n",
    "df = pd.read_csv('processed_df.csv')\n",
    "df_t = pd.read_csv('text_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing data in df\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bivariate Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stacked barcharts function for categorical variables\n",
    "def stacked_barcharts(dataframe, columns, app_step, fig_title, fig_height, rel_freq=None, ncols=5):\n",
    "    nrows = 1 + (len(columns) - 1) // ncols\n",
    "    fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(14, nrows * fig_height))\n",
    "    for i, column in enumerate(columns):\n",
    "        if rel_freq == True:\n",
    "            # calculate pivot table\n",
    "            pivot = pd.crosstab(dataframe[app_step], dataframe[column])\n",
    "            # divide by column sums to get frequency per column\n",
    "            freq = pivot.div(pivot.sum())\n",
    "            # display as stacked bar chart with 100%\n",
    "            ax = axes[i // 5, i % 5] if nrows > 1 else axes[i]\n",
    "            with warnings.catch_warnings():\n",
    "                warnings.simplefilter('ignore')\n",
    "                freq.transpose().plot(kind='bar', ax=ax, stacked=True, legend=False)\n",
    "        elif rel_freq == False:\n",
    "            # calculate pivot table\n",
    "            pivot = pd.crosstab(dataframe[app_step], dataframe[column])\n",
    "            ax = axes[i // 5, i % 5] if nrows > 1 else axes[i]\n",
    "            with warnings.catch_warnings():\n",
    "                warnings.simplefilter('ignore')\n",
    "                pivot.transpose().plot(kind='bar', ax=ax, stacked=True, legend=False)\n",
    "        else:\n",
    "            print('Please specify rel_freq=True or rel_freq=False')\n",
    "    for i in range(len(columns), nrows * 5):\n",
    "        ax = axes[i // 5, i % 5] if nrows > 1 else axes[i]\n",
    "        fig.delaxes(ax)\n",
    "    fig.suptitle(fig_title)\n",
    "    plt.tight_layout(2.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"2021 DATA:\\n\")\n",
    "# Total number of candidates in Progress_5_Start_1stDay\n",
    "total_1stDay = df.Progress_5_Start_1stDay.value_counts()\n",
    "print('Total number of people at this stage: {:,}'.format(total_1stDay.sum()))\n",
    "# Percent of people that started their first day (Progress_5_Start_1stDay == 1)\n",
    "percent_1stDay = total_1stDay[1] / total_1stDay.sum() * 100\n",
    "print('Percent of people that started their first day: {:.2f}% ({:,})'.format(percent_1stDay, total_1stDay[1]))\n",
    "# Percent of people that did not start their first day (Progress_5_Start_1stDay == 0)\n",
    "percent_not_1stDay = total_1stDay[0] / total_1stDay.sum() * 100\n",
    "print('Percent of people that did not made it to this step: {:.2f}% ({:,})'.format(percent_not_1stDay, total_1stDay[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creaste new df with the people started their first day (Progress_5_Start_1stDay = 1)\n",
    "df_1stDay = df[df['Progress_5_Start_1stDay'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stacked bar of Progress_5_Start_1stDay and Progress_6_Complete_2yrs\n",
    "plt.close('all')\n",
    "plt.figure(figsize=(4,3))\n",
    "ax = plt.subplot()\n",
    "df_counts = df_1stDay.groupby(['Progress_5_Start_1stDay', 'Progress_6_Complete_2yrs']).size().unstack()\n",
    "df_counts.plot(kind='bar', stacked=True, ax=ax, legend=True)\n",
    "plt.title('Start_1stDay vs Complete_2yrs')\n",
    "plt.xlabel('Progress_5_Start_1stDay')\n",
    "plt.ylabel('Count')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "df_1stDay_2yrs = df_1stDay[df_1stDay['Progress_6_Complete_2yrs'] == 1].shape[0] / total_1stDay[1] * 100\n",
    "print('Out of the {:,} people that started their first day, {:.2f}% ({:,}) completed the 2 years program'.format(total_1stDay[1], df_1stDay_2yrs ,df_counts[1][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group columns based on their information\n",
    "col_list_s = [column for column in df_1stDay.columns if len(df_1stDay[column].unique()) <= 10]\n",
    "col_list_l = [column for column in df_1stDay.columns if len(df_1stDay[column].unique()) >= 10]\n",
    "#print(col_list_s)\n",
    "#print('\\n')\n",
    "#print(col_list_l)\n",
    "\n",
    "cat_cols_s = ['career_level', 'UG_school_selectivity', 'UG_major_minor_STEM', 'UG_sports', 'UG_PellGrant', 'Leadership_role', 'family_responsibility', 'filled_cols_count', 'filled_cols_count_bin', 'steps_completed', 'prev_LIC_level', 'Ivy_league', 'app_number', 'high_profile']\n",
    "cat_cols_l = ['Region']\n",
    "num_cols = ['App_start_date', 'App_submit_date', 'ConfirmOffer_date', 'UG_GPA', 'Cost', 'app_completion_days', 'financial_gap','avg_dimension_score', 'offer_delay_days']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.close('all')\n",
    "plt.figure(figsize=(6, 5))\n",
    "ax = plt.subplot()\n",
    "\n",
    "df_counts = df_1stDay.groupby(['filled_cols_count_bin', 'Progress_6_Complete_2yrs']).size().unstack()\n",
    "\n",
    "# Calculate row totals and sort the DataFrame by row totals in descending order\n",
    "df_counts['Total'] = df_counts.sum(axis=1)\n",
    "df_counts.sort_values('Total', ascending=False, inplace=True)\n",
    "df_counts.drop(columns='Total', inplace=True)\n",
    "# create frequency of df_counts\n",
    "df_counts = df_counts.div(df_counts.sum(axis=1), axis=0)\n",
    "df_counts.plot(kind='bar', stacked=True, ax=ax, legend=True)\n",
    "# add x tick labels with \"Complete application form for 1 and Incomplete application form for 0\"\n",
    "ax.set_xticklabels(['Complete application form', 'Incomplete application form'])\n",
    "# rotate labels 0 degrees\n",
    "plt.xticks(rotation=0)\n",
    "\n",
    "plt.title('Complete Application Form vs Candidate Completed 2yrs')\n",
    "plt.xlabel('Progress_6_Complete_2yrs')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.close('all')\n",
    "plt.figure(figsize=(6, 5))\n",
    "ax = plt.subplot()\n",
    "\n",
    "df_counts = df_1stDay.groupby(['high_profile', 'Progress_6_Complete_2yrs']).size().unstack()\n",
    "\n",
    "# Calculate row totals and sort the DataFrame by row totals in descending order\n",
    "df_counts['Total'] = df_counts.sum(axis=1)\n",
    "df_counts.sort_values('Total', ascending=False, inplace=True)\n",
    "df_counts.drop(columns='Total', inplace=True)\n",
    "# create frequency of df_counts\n",
    "df_counts = df_counts.div(df_counts.sum(axis=1), axis=0)\n",
    "df_counts.plot(kind='bar', stacked=True, ax=ax, legend=True)\n",
    "# add x tick labels with \"Complete application form for 1 and Incomplete application form for 0\"\n",
    "ax.set_xticklabels([0, 1, 2, 3])\n",
    "# rotate labels 0 degrees\n",
    "plt.xticks(rotation=0)\n",
    "\n",
    "plt.title('High Profile Candidate vs Candidate Completed 2yrs')\n",
    "plt.xlabel('Progress_6_Complete_2yrs')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Categorical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph cat_cols_s by Progress_6_Complete_2yrs\n",
    "stacked_barcharts(dataframe=df_1stDay, columns=cat_cols_s, app_step='Progress_6_Complete_2yrs', fig_title='Progress_6_Complete_2yrs', fig_height=3, rel_freq=True, ncols=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close('all')\n",
    "plt.figure(figsize=(15, 5))\n",
    "ax = plt.subplot()\n",
    "\n",
    "df_counts = df_1stDay.groupby(['Region', 'Progress_6_Complete_2yrs']).size().unstack()\n",
    "df_counts.rename(columns={0:'Incomplete', 1:'Complete'}, inplace=True)\n",
    "# create frequency of df_counts\n",
    "df_counts = df_counts.div(df_counts.sum(axis=1), axis=0)\n",
    "df_counts.sort_values('Incomplete', ascending=False, inplace=True)\n",
    "df_counts.plot(kind='bar', stacked=True, ax=ax, legend=True)\n",
    "\n",
    "plt.title('Percent of candidates who completed the program by region')\n",
    "plt.xlabel('Progress_6_Complete_2yrs')\n",
    "plt.ylabel('Percentage of candidates [%]')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Numerical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_histograms(dataframe, column_list, graph_type='hist', density=None):\n",
    "    nrows = len(column_list)\n",
    "    fig, axes = plt.subplots(nrows=nrows, ncols=1, figsize=(10, 3 * nrows))\n",
    "    for i, col in enumerate(column_list):\n",
    "        if graph_type == 'hist':\n",
    "            ax = axes[i]\n",
    "            ax.hist(dataframe[dataframe['Progress_6_Complete_2yrs'] == 0][col], bins=20, density=density, alpha=0.5, label='Did not complete 2 years')\n",
    "            ax.hist(dataframe[dataframe['Progress_6_Complete_2yrs'] == 1][col], bins=20, density=density, alpha=0.5, label='Completed 2 years')\n",
    "            ax.set_title(f\"{col} vs completed 2 years\")\n",
    "            ax.set_xlabel(col)\n",
    "            ax.set_ylabel('Count')\n",
    "            ax.legend()\n",
    "        elif graph_type == 'kde':\n",
    "            ax = axes[i]\n",
    "            sns.kdeplot(dataframe[dataframe['Progress_6_Complete_2yrs'] == 0][col], ax=ax, label='Did not complete 2 years')\n",
    "            sns.kdeplot(dataframe[dataframe['Progress_6_Complete_2yrs'] == 1][col], ax=ax, label='Completed 2 years')\n",
    "            ax.set_title(f\"{col} vs completed 2 years\")\n",
    "            ax.set_xlabel(col)\n",
    "            ax.set_ylabel('Density')\n",
    "            ax.legend()\n",
    "        else:\n",
    "            print(\"Invalid graph type\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(10, 3))\n",
    "\n",
    "sns.kdeplot(df_1stDay[df_1stDay['Progress_6_Complete_2yrs'] == 0]['Cost'], ax=ax, label='Did not complete 2 years')\n",
    "sns.kdeplot(df_1stDay[df_1stDay['Progress_6_Complete_2yrs'] == 1]['Cost'], ax=ax, label='Completed 2 years')\n",
    "ax.set_title('Avg. cost of living vs completed 2 years')\n",
    "ax.set_xlabel('Average Cost of Living in Region [USD]')\n",
    "ax.set_ylabel('Density')\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols = ['App_start_date', 'App_submit_date', 'ConfirmOffer_date', 'UG_GPA', 'Cost', 'app_completion_days', 'financial_gap','avg_dimension_score', 'offer_delay_days']\n",
    "num_cols = ['UG_GPA', 'Cost', 'financial_gap','avg_dimension_score', 'App_start_date', 'App_submit_date', 'ConfirmOffer_date', 'app_completion_days', 'offer_delay_days']\n",
    "plot_histograms(dataframe=df_1stDay, column_list=num_cols, graph_type='hist', density=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix\n",
    "cor_df = df_1stDay.drop(columns=['variable', 'filled_cols_count_bin', 'family_responsibility', 'financial_gap', 'steps_completed', 'Progress_1_Invite_Intrvw', 'Progress_2_Complete_Intrvw', 'Progress_3_Accepted_toCorp', 'Progress_4_Confirm_Offer', 'Progress_5_Start_1stDay'])\n",
    "progress_col = cor_df['Progress_6_Complete_2yrs']\n",
    "cor_df.drop(columns=['Progress_6_Complete_2yrs'], inplace=True)\n",
    "cor_df['Progress_6_Complete_2yrs'] = progress_col\n",
    "# calculate correlation matrix\n",
    "corr = np.abs(cor_df.corr())\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "cmap = sns.color_palette('magma')\n",
    "sns.heatmap(corr, cmap=cmap, square=True)\n",
    "plt.title('Correlation between numerical df: abs values')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hypothesis testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run chi2 test on filled_cols_count_bin and Progress_6_Complete_2yrs\n",
    "chi2, pval, dof, ex = chi2_contingency(pd.crosstab(df_1stDay['filled_cols_count_bin'], df_1stDay['Progress_6_Complete_2yrs']))\n",
    "print('p-value:', pval)\n",
    "print('\\n')\n",
    "print('Null: The variables are independent. No relationship exists.')\n",
    "print('Alternative: A relationship between the variables exists.\\n')\n",
    "print('Pval is less than 0.05, so we reject the null hypothesis. There is a relationship between filled_cols_count_bin and Progress_6_Complete_2yrs')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Kolmogorov-Smirnov test on cost vs progress_6_complete_2yrs\n",
    "from scipy.stats import ks_2samp\n",
    "stat, pval = stats.ks_2samp(df_1stDay[df_1stDay.Progress_6_Complete_2yrs == 0].Cost, df_1stDay[df_1stDay.Progress_6_Complete_2yrs == 1].Cost)\n",
    "print('p-value:', pval)\n",
    "print('\\n')\n",
    "print('Null: The variables are independent. No relationship exists.')\n",
    "print('Alternative: A relationship between the variables exists.\\n')\n",
    "print('Pval is less than 0.05, so we reject the null hypothesis. There is a relationship between filled_cols_count_bin and Progress_6_Complete_2yrs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run chi2 test on high_profile and Progress_6_Complete_2yrs\n",
    "chi2, pval, dof, ex = chi2_contingency(pd.crosstab(df_1stDay['high_profile'], df_1stDay['Progress_6_Complete_2yrs']))\n",
    "print('p-value:', pval)\n",
    "print('\\n')\n",
    "print('Null: The variables are independent. No relationship exists.')\n",
    "print('Alternative: A relationship between the variables exists.\\n')\n",
    "print('Pval is greater than 0.05, so we fail to reject the null hypothesis. There is no relationship between high_profile and Progress_6_Complete_2yrs')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter people that started their first day in Progress_5_Start_1stDay\n",
    "df = df[df['Progress_5_Start_1stDay'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check nulls in df and unique values in df of categories with 6 or less unique values\n",
    "{column : ['Nulls: ' + str(df[column].isnull().sum()), 'Unique values: ' + str(df[column].nunique())] for column in df.columns}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill GPA missing values with the mean of the column\n",
    "df['UG_GPA'].fillna(df['UG_GPA'].mean(), inplace=True)\n",
    "# Fill region missing values with the mode of the column\n",
    "#df['Region'].fillna(df['Region'].mode()[0], inplace=True)\n",
    "# Fill cost missing values with the mean of the column\n",
    "df['Cost'].fillna(df['Cost'].mean(), inplace=True)\n",
    "# Fill financial gap missing values with the mean of the column\n",
    "df['financial_gap'].fillna(df['financial_gap'].mean(), inplace=True)\n",
    "# Fill avg_dimension_score missing values with the mean of the column\n",
    "df['avg_dimension_score'].fillna(df['avg_dimension_score'].mean(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One hot encode Region column\n",
    "#df = pd.get_dummies(df, columns=['Region'], prefix=['Region'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create labels and features\n",
    "labels = df['Progress_6_Complete_2yrs']\n",
    "features = df.drop(['offer_delay_days', 'Leadership_role', 'Ivy_league', 'app_number', 'Region', 'avg_dimension_score', 'financial_gap', 'App_start_date', 'App_submit_date', 'ConfirmOffer_date', 'steps_completed', 'variable', 'Progress_1_Invite_Intrvw', 'Progress_2_Complete_Intrvw', 'Progress_3_Accepted_toCorp', 'Progress_4_Confirm_Offer', 'Progress_5_Start_1stDay', 'Progress_6_Complete_2yrs'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{column : 'Nulls: ' + str(features[column].isnull().sum()) for column in features.columns }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine which features are relevant for training.\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "threshold_n=0.95\n",
    "sel = VarianceThreshold(threshold=(threshold_n* (1 - threshold_n) ))\n",
    "sel_var=sel.fit_transform(features)\n",
    "features[features.columns[sel.get_support(indices=True)]] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split training and test sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(features, labels, test_size = 0.20, random_state = 1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train the model with pipeline\n",
    "lr_pipe = make_pipeline(StandardScaler(), LogisticRegression(max_iter=1000, class_weight='balanced'))\n",
    "lr_pipe.fit(x_train, y_train)\n",
    "# Predictions\n",
    "lr_predictions = lr_pipe.predict(x_test)\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, lr_predictions)\n",
    "ax = sns.heatmap(cm, linewidth = 0.5, annot = True, cmap = 'Blues', fmt = 'g') \n",
    "bottom, top = ax.get_ylim()\n",
    "ax.set_ylim(bottom + 0.5, top - 0.5)\n",
    "plt.ylabel('Predicted values')\n",
    "plt.xlabel('Real values')\n",
    "plt.show()\n",
    "# Model score\n",
    "print('Train Score: ', lr_pipe.score(x_train, y_train))\n",
    "print('Test Score: ', lr_pipe.score(x_test, y_test))\n",
    "# Classification Report\n",
    "print(classification_report(y_test, lr_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Permutation importances\n",
    "result = permutation_importance(lr_pipe, x_test, y_test, n_repeats=10, random_state=42, n_jobs=2)\n",
    "\n",
    "sorted_importances_idx = result.importances_mean.argsort()\n",
    "importances = pd.DataFrame(result.importances[sorted_importances_idx].T, columns=features.columns[sorted_importances_idx],)\n",
    "# create figure and set size\n",
    "#fig, ax = plt.subplots(figsize=(6, 15))\n",
    "ax = importances.plot.box(vert=False, whis=10)\n",
    "ax.set_title(\"Permutation Importances (test set)\")\n",
    "ax.axvline(x=0, color=\"k\", linestyle=\"--\")\n",
    "ax.set_xlabel(\"Decrease in accuracy score\")\n",
    "ax.figure.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph importances from coefficients\n",
    "lr = lr_pipe.steps[1][1]\n",
    "importances = pd.DataFrame(data={\n",
    "    'Attribute': x_train.columns,\n",
    "    'Importance': lr.coef_[0]\n",
    "})\n",
    "importances = importances.sort_values(by='Importance', ascending=False)\n",
    "# Plot results\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.bar(x=importances['Attribute'], height=importances['Importance'], color='dodgerblue')\n",
    "plt.title('Feature importances obtained from coefficients')\n",
    "plt.xticks(rotation='vertical')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train random forests classifier\n",
    "rf = RandomForestClassifier(n_estimators=300, max_depth=10, class_weight='balanced')\n",
    "rf.fit(x_train, y_train)\n",
    "# Predictions\n",
    "rf_predictions = rf.predict(x_test)\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, rf_predictions)\n",
    "ax = sns.heatmap(cm, linewidth = 0.5, annot = True, cmap = 'Greens', fmt = 'g') \n",
    "bottom, top = ax.get_ylim()\n",
    "ax.set_ylim(bottom + 0.5, top - 0.5)\n",
    "plt.ylabel('Predicted values')\n",
    "plt.xlabel('Real values')\n",
    "plt.show()\n",
    "# Model score\n",
    "print('Train Score: ', rf.score(x_train, y_train))\n",
    "print('Test Score: ', rf.score(x_test, y_test))\n",
    "# Classification Report\n",
    "print(classification_report(y_test, rf_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Permutation importances\n",
    "result = permutation_importance(rf, x_test, y_test, n_repeats=10, random_state=42, n_jobs=2)\n",
    "\n",
    "sorted_importances_idx = result.importances_mean.argsort()\n",
    "importances = pd.DataFrame(result.importances[sorted_importances_idx].T, columns=features.columns[sorted_importances_idx],)\n",
    "# create figure and set size\n",
    "ax = importances.plot.box(vert=False, whis=10)\n",
    "ax.set_title(\"Permutation Importances (test set)\")\n",
    "ax.axvline(x=0, color=\"k\", linestyle=\"--\")\n",
    "ax.set_xlabel(\"Decrease in accuracy score\")\n",
    "ax.figure.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph importances from coefficients\n",
    "importances = pd.DataFrame(data={\n",
    "    'Attribute': x_train.columns,\n",
    "    'Importance': rf.feature_importances_\n",
    "})\n",
    "importances = importances.sort_values(by='Importance', ascending=False)\n",
    "# Plot results\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.bar(x=importances['Attribute'], height=importances['Importance'], color='mediumseagreen')\n",
    "plt.title('Feature importances obtained from coefficients')\n",
    "plt.xticks(rotation='vertical')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note: The tendency of this approach is to inflate the importance of continuous features or high-cardinality categorical variables*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K-Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train the model with pipeline\n",
    "knn_pipe = make_pipeline(StandardScaler(), KNeighborsClassifier(n_neighbors = 100))\n",
    "knn_pipe.fit(x_train, y_train)\n",
    "# Predictions\n",
    "knn_predictions = knn_pipe.predict(x_test)\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, knn_predictions)\n",
    "ax = sns.heatmap(cm, linewidth = 0.5, annot = True, cmap = 'Reds', fmt = 'g') \n",
    "bottom, top = ax.get_ylim()\n",
    "ax.set_ylim(bottom + 0.5, top - 0.5)\n",
    "plt.ylabel('Predicted values')\n",
    "plt.xlabel('Real values')\n",
    "plt.show()\n",
    "# Model score\n",
    "print('Train Score: ', knn_pipe.score(x_train, y_train))\n",
    "print('Test Score: ', knn_pipe.score(x_test, y_test))\n",
    "# Classification Report\n",
    "print(classification_report(y_test, knn_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Permutation importances\n",
    "result = permutation_importance(knn_pipe, x_test, y_test, n_repeats=10, random_state=42, n_jobs=2)\n",
    "\n",
    "sorted_importances_idx = result.importances_mean.argsort()\n",
    "importances = pd.DataFrame(result.importances[sorted_importances_idx].T, columns=features.columns[sorted_importances_idx],)\n",
    "# create figure and set size\n",
    "ax = importances.plot.box(vert=False, whis=10)\n",
    "ax.set_title(\"Permutation Importances (test set)\")\n",
    "ax.axvline(x=0, color=\"k\", linestyle=\"--\")\n",
    "ax.set_xlabel(\"Decrease in accuracy score\")\n",
    "ax.figure.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "805ab787e7b2c169ffcd54665a62f6737584bdbffb8579ace331327af8e42d4f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
